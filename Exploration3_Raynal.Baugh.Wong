---
  title: 'Exploration 3: Matrices make life easier.'
author: "PS531 Group: Isabella Raynal, James Baugh, Adrian Wong"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
  number_sections: true
fig_caption: yes
latex_engine: xelatex
citation_package: biblatex
keep_tex: true
fig_height: 8
fig_width: 8
graphics: yes
geometry: "left=1.25in,right=1.25in,top=1in,bottom=1in"
fontsize: 11pt
bibliography: classbib.bib
biblio-style: authoryear-comp
---
  
  <!-- Make this document using library(rmarkdown); render("exploration2.Rmd") -->
  \input{mytexsymbols}

```{r setup, echo=FALSE, results=FALSE, include=FALSE, cache=FALSE}
library(here)
library(tidyverse)
library(readstata13)
library(estimatr)
source(here::here("rmd_setup.R"))
```


Imagine that you receive a WhatsApp from old friend. He is from one of the new
political analytics firms that started to grow awhile back during the first
Obama campaign in the USA and he has a prediction problem. He has a small
dataset of 8 cities and would like to predict current voting turnout in those
cities based on a complex model. He says, "My last analyst provided the
following code to fit my model but then stopped. I think that the best model of
voting turnout uses median household income, median age, racial diversity
(here, percent african american), and the number of candidates running for this
city council office. I told the analyst this model and he provided the
following code. Can you help me?"


```{r getthedata}
news.df <- read.csv("~/Downloads/news.df.csv")
## Uncomment out the following line for your own use to download the csv from the web
## news.df<-read.csv("http://jakebowers.org/Data/news.df.csv")
news.df$sF <- factor(news.df$s)
```

"I really don't understand the following `lsCriterion` function. Can you write out the criterion using math and explain it to me in plain language? I'm always especially interested in understanding **why** these stats types are doing this stuff, and I'm so grateful that you can explain it simply and plainly to me."

```{r}
lsCriterion <- function(b, y, X) {
  yhat <- b[1] * X[, 1] + b[2] * X[, 2] + b[3] * X[, 3] + b[4] * X[, 4] + b[5] * X[, 5]
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}
X <- as.matrix(cbind(constant = 1, news.df[, c("medhhi1000", "medage", "blkpct", "cands")]))
y <- news.df$rpre
```
# Answer: lsCriterion is the equation for lm, it creates its own functions using the variables b, y, X.  X is the predictor variable (the input) and y is the response variable (the output).  b is the coefficient that we multiply times X.  It looks like it is creating a linear regression in the first line in which yhat is the predicted outcome value that is calculated based of the b coefficients and X values.  ehat takes the difference between y (the actual outcome) and yhat (the expected outcome).  thessr takes the sum of all of the ehat squared values.  The reason we are squaring ehat is because some of the differences may be negative (when the predicted value is larger than the actual value) or positive (when the predicted value is less than the actual value) and squaring it keeps the sign constant.  return(thessr) means that the function will output the value of the sum of the squared differences between the expected and actual values.

#After the function, we can see that X and y are defined.  X is defined as a Matrix that combines certain columns in the data set.  These columns are what the old friend thinks is the best model of voting turnout: 
#medhhi1000: median household income (in thousands)
#medage: median age
#blkpct: racial diversity (percentage black)
#cands: number of candidates running for city council office

#We can see that in the equation for yhat there are 5 X*b: 4 are for the 4 explanatory variables I just mentioned and 1 is the intercept

#y is defined as the predicted current voting turnout.  

#To summarize, this model is trying to show how the predicted voting turnout based on median household income, median age, percentage black, and number of candidates running compares with the actual voting turnout. 

#One side note: What I believe is that this function is finding the sum of squares residual for the hypothetical model when given a series of guesses for the intercept and coefficients for each variable (‘b’), the baseline, observed voter turnout (‘y’, which in this case is the variable ‘rpre’), and the values of the variables in the model (‘X’).
#The Sum of Squares Residual, as I understand it, is useful in determining the potential accuracy of a model. The larger the Residual, the less accurate the model. 

"He said to 'try some different vectors' and I think that this meant that I was to guess about the values for the coefficients in the model and that, after trying a bunch, I would choose the vector that I liked best. So, for example, I tried a model with all zeros:"

```{r}
lsCriterion(c(0, 0, 0, 0, 0, 0), y = y, X = X)
```
#ANSWER: this gives sum(y^2), since yhats are all zero. The result when all the coefficients are zero is 6,329.  I believe this is the intercept of the linear regression, which would mean that the predicted voter turnout if none of the variables had an effect would be 6,329 people.  We can compare this to the actual voter turnout of 228 people.  This seems to be way off.

# Other note: The result being way off means the model is also probably way off. If 228 is assumed as a baseline for the actual result of a good model, a residual of 6,329 would indicate that the line is nowhere close to correct. In a model that fits well, the residual would be substantially lower than this.

And then tried to see a bunch of other models.

```{r}
lsCriterion(c(1, 0, 4, 0, 0, 0), y = y, X = X) ## hmm..ok... can I do better? ## This is even more way off at over 90,000.
set.seed(12345)
lsCriterion(runif(5, -100, 100), y = y, X = X) ## bad ##runif() is assuming we have a uniform distribution and taking a random sample from that distribution
```
# the runif selected 5 values randomly between -100 and 100 -- We CAN, theoretically, do better, but the better question is, would it be a realistic set of assumptions? 

"After trying a bunch of models, however, I started to get tired. Can you help me do this faster? I asked the analyst (who is really a construction engineer and not a statistician) if I could use a loop but he said it would be better to 'try to optimize the objective function' and this is as far as I got."


```{r}
lsSolution <- optim(
  fn = lsCriterion, par = c(0, 0, 0, 0, 0), X = X, y = y,
  method = "BFGS", control = list(trace = 1, REPORT = 1)
)
```

"Is this the best solution? How well does this model predict the actual outcome (the variable ` r `) (this model uses baseline turnout or `rpre`). Can you give me some quantitative measure of how well our model predicted the outcome? I think that you can use the code from the `lsCriterion` function to develop predictions and compare our predictions to the actual turnout observed (the variable ` r`), right?"

#ANSWER: Seems like it did not predict well because the sum of the squared residuals is larger for the "actual outcome" than for the baseline turnout. *see below -- So, the above optim function attempts to minimize the sum of squares residual score, thereby maximizing the sum of square regression, and the accuracy of the regression line. Because ‘y’ in this case is defined as the variable ‘rpre’, which is the baseline voter turnout, this function should, in theory, predict quite closely (if not perfectly) the coefficients and intercept of a linear regression model, with ‘rpre’ as the dependent variable, and the other identified variables as control variables. We can compare the values here:

```{r}
test1 <- lm(rpre ~ medhhi1000+medage+blkpct+cands, data=news.df)
cbind(lsSolution$par, test1$coefficients) ## And indeed, they appear to be identical. -- If we wanted to test how the optim function can predict the actual results (the variable ‘r’), we need to re-define ‘y’, and then re-run the function. We can then compare the results to another linear regression model, to see, quantitatively, how the prediction stands up. 

test2 <- lm(r ~ medhhi1000+medage+blkpct+cands, data=news.df)
lsSolution2 <- optim(fn = lsCriterion, 
               par = c(0, 0, 0, 0, 0), 
               X = X, 
               y = news.df$r, #redefining y 
               method = "BFGS", 
               control = list(trace = 1, REPORT = 1))

cbind(lsSolution2$par, test2$coefficients) # they are equal, which means it predicts it perfectly

lsTest <- function(b, y1, X) {
  yhat <- b[1] * X[, 1] + b[2] * X[, 2] + b[3] * X[, 3] + b[4] * X[, 4] + b[5] * X[, 5]
  ehat <- y1 - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}
X <- as.matrix(cbind(constant = 1, news.df[, c("medhhi1000", "medage", "blkpct", "cands")]))
y1 <- news.df$r

lsTest(c(lsSolution$par), y = y1, X = X)
```

"Now, I wanted to add another variable to see how well that new model fit. For example, maybe `blkpct` has a curvilinear relationship with the outcome. I complained to the analyst that I would have to re-write the function every time that I had a new model. So, the analyst said, 'You don't have to do that, just use matrices.' and he sent this:"

```{r}
bVector <- solve(t(X) %*% X) %*% t(X) %*% y ##the equation for Ordinary Least Squares
yhat <- X %*% bVector ##sum multiplication
ehat <- y - yhat ## actual minus predicted based on sum multiplication
summary(ehat)
```

"Now, I'm very impressed at the speed and conciseness of this! I mean, he got the same vector of coefficients in like 1/1000 the time that it took me to search for a solution --- even using a fast optimizer. Also, he got the predictions very quickly too! But I'm confused about how this works. I understand the idea of proposing different values for the different coefficients and then asking how well they do --- in a sum of squared error sense. But the three lines that create `bVector` and `yhat` and even `ehat` are a mystery and I worry that they are not actually comparing my predictions of past turnout to observed future turnout (`rpre` versus ` r`). Maybe you can help? I asked the analyst to provide a little guidance to help you get practice with these ideas."

#NOTE: bVector = lsSolution, which is the parameter weights

So, you need to be able to explain what is happening when we tell R to do least squares with the matrix $\bX$ and
vector $\by$ via $(\bX^{T}\bX)^{-1}\bX^{T}\by$ using the command
`solve(t(X)%*%X)%*%(t(X)%*%y)`. Where
`t(X)`$\equiv \bX^{T}$ (meaning the transpose of $\bX$) and
`solve(X)`$\equiv \bX^{-1}$ (meaning the inverse of $\bX$).

Here are some steps you might take to produce this explanation.

1.  First, let's create a $\bX$ matrix using the newspapers data to
  do a regression of the form `baseline.turnoutx~income+median age` (where
  baseline.turnout is `rpre` and income is `medhhi1000` and median age of the city is `medage`). Here is
  how one might do this in R for both $\bX$ and $\by$ (where,
  \textbf{bold} represents matrices or vectors):

```{r setupXandy, results='hide'}
X <- as.matrix(cbind(1, news.df$medhhi1000, news.df$medage)) # "bind" together columns from the data and an intercept
y <- matrix(news.df$rpre, ncol = 1) # not strictly necessary, y<-news.df$rpre would also work
X
y
# Structure of the objects
str(X)
str(y)
# Look at the dimensions of the objects: number rows by number columns
dim(X)
dim(y)
```
   - Explain how we created the $\bX$ matrix \emph{Hint:} The column of 1s has to do with the intercept or constant term.
   
#ANSWER: We created the $\bX$ matrix by binding three vector columns together: the 1's for the intercept, the income for the focal covariate and age for the control covariate. -- Fox (2008) chapter 9 shows that 1 always appears in each row of the first column in the X matrix in the linear equation y = X * b + e


   - What do the columns of $\bX$ represent?
   
#ANSWER: The column of 1s represents the response variable y. It holds a place in the matrix for a coefficient when "no covariates are taken into account".

# Another way to think of it: The 1’s are just placeholders I think? The model matrix, X, needs to have a column for the intercept outcome. 


the columns represent the values of covariate for each observation, in this case, each city on whatever time scale was used to measure the phenomena (average by year for income? or "at time of rpre"?)

   - What do the rows represent?
   
# ANSWER: If $\bx$ is just the matrix ‘X’, then the rows are each of the observations in the dataset, i.e. each city, and the columns are the covariates.

2. First, addition and subtraction: Try each of the following lines of math in R and explain to yourself (or your colleagues) what happened, and what this means about matrix math. I did the first one as an example.

  Explain what is happening with each of the following
```{r addition,echo=TRUE,results='markup'}
X + 2 # adding 2 to each value in the matrix
X - 5 # subtracting 5 from each
```

"When you add a single number (aka a scalar) to a matrix, the scalar is added to each entry in the matrix."

Notice: If we didn't have matrix math, here is what we'd have to do to
add a scalar to a matrix
```{r add2,echo=TRUE,tidy=TRUE}
Xplus2 <- matrix(NA, nrow = 8, ncol = 3) # explain
#ANSWER: creates a dumby matrix with the number of rows and columns we will want to populate with values, in this case 8 rows, 3 columns.
for (row.entry in 1:8) { # explain
  for (col.entry in 1:3) { # explain
    Xplus2[row.entry, col.entry] <- X[row.entry, col.entry] + 2 # explain
  }
}
#ANSWER: this for loop tells the computer that for each of the rows of the matrix 1:8, and for each the columns of the matrix 1:3, Xplus2 is to be set to the sum of 2 and the corresponding value in X, repeated for each row and column.
(X + 2) == Xplus2 #  explain
# ANSWER: this code asks to computer to check if each value in the Xplus2 matrix is equal to each value calculated by the computer by performing the matrix addition of X + 2
# An easier check on whether two objects are the same (except for
# names and such)
all.equal((X + 2), Xplus2, check.attributes = FALSE)
X - 2 # Explain
# ANSWER: performs matrix subtraction, subtracting 2 from each row and column of the X matrix.
```

```{r vecmatadd,echo=TRUE}
twovec <- matrix(c(2, 2, 2), nrow = 1, ncol = 3) # explain - ANSWER: creates a matrix with 1 row and three columns, populated with the value 2 for each.
twomat <- matrix(2, nrow = 8, ncol = 3) # explain - ANSWER: creates a matrix with 8 rows, 3 columns, populated with the value 2 for each.
```

You'll see some errors appear below. Your job is to explain why R
failed or made an error. I had to surround these lines in `try()`
to prevent R from stopping at the error.

```{r vecmatadd2,echo=TRUE,message=TRUE, warning=TRUE, error=TRUE}
try(X + twovec) # explain - Matrix addition or subtraction can only be performed when the number of rows and columns are equivalent for each matrix. --  the above codes is trying to add two matrices together.  X is an 8x3 matrix and twovec is a 1x3 matrix.  Matrices can only be added when they have the same dimensions (for example, two matrices that are both 8x3 or that are both 1x3).  The reason we are getting an error is because X and twovec have different dimensions, and therefore cannot be added.
try(X + t(twovec)) # Here, you need to explain what t(twovec) does. - ANSWER: t(Matrix) transforms the matrix by reflecting the values across the diagonal, creating a 3x1 matrix from a 1x3 matrix. --  t(twovec) transposes the matrix twovec, meaning that it reverses the rows and the columns.  t(twovec) turns the matrix from having 1 row and 3 columns to having 3 rows and 1 column
##  This is the same issue as above where the code is trying to add two matrices that do not have the same dimensions and therefore cannot be added.

```

```{r matmat,echo=TRUE,results='hide',tidy.opts=list(keep.comment=TRUE)}
X + twomat # explain - ANSWER: same as performing X + 2, matrix addition adds corresponding values together. - the above code works because it is adding two matrices with the same dimensions: 8x3.  This takes each number in one matrix and adds it to the corresponding number in the second matrix.  For example, the number in the first row and first column of X is added to the number in the first row and first column of two mat, and so on.
(X + twomat) == (X + 2) # explain --  Does adding the matrix twomat to matrix X give us the same results as if we added the scalar number 2 to matrix X?  Yes, because both or ways to add the number 2 to each value in matrix X.
all.equal((X + twomat), (X + 2), check.attributes = FALSE) # explain - proves above, it is the same as what we had before seeing if X + twomat gives us the same results as X + 2.  However, the format of the output is a little different.  (X + twomat) == (X + 2) gives us a matrix of 24 TRUEs (there is a true in each spot on the matrix where the two forms of addition gave us the same answer), whereas the all.equal() function gives us a singular TRUE that the entire matrix is the same.  This would suggest that (X + twomat) == (X + 2) gives us a more detailed answer because it shows which values are TRUE vs. FALSE, whereas all.equal() just says TRUE if the entire matrix is the same but FALSE if it is different.  If as.equal() produced an answer of FALSE, we do not know which of the values in the matrix are different or how many (ex. We could get FALSE if we had two totally different matrices, but we could also get FALSE if only one value was different).all.equal((X + twomat), (twomat + X), check.attributes = FALSE) # explain - proves that Matrix addition follows the commutative property, i.e. A + B == B + A
## the code above is asking something different here.  It is asking whether the order in which we add the matrices X and twomat matters.  The answer is yes because of the commutative property of addition
```

```{r self,results='hide',tidy.opts=list(keep.comment=TRUE)}
X + X # why does this work? - Because X has the same dimensions as itself. 
```

3. Second, multiplication. Notice that the symbols for scalar
multiplication and matrix multiplication are not the same. Try each
of the following lines of math in R and explain to yourself (or
                                                             your colleagues) what happened, and what this means about matrix
math.

```{r multiplication,results='hide',tidy.opts=list(keep.comment=FALSE)}
X * 2 # Explain - scalar multiplication follows the distributive property.
all.equal((X * 2), (2 * X)) # explain - matrix multiplication is commutative.
X^2 #  # explain - X^2 does not equal X * X, instead, it distributes the squared operator to each value within the matrix.
X^.5 # # explain - same property applies with square root.
sqrt(X) # # explain - this is another way to write ^.5 and functions in r.
```

Now, let's get a vector of coefficients to make matrix math link even more
tightly with what we've already done fitting models to data:
  
  ```{r}
b <- solve(t(X) %*% X) %*% t(X) %*% y # explain - this matrix equation produces the parameters of the linear regression, the weighted coefficients for the model. ## ordinary least squares equation.  According to Fox (2008) “t(X) %*% X) contains sums of squares and products among the regressors” and “t(X) %*% y contains sums of cross products between the regressors and the response variable” (chapter 9).
  
dim(b) # explain - b is the vector of parameters.
```

Now, let's do matrix multiplication the tedious way.  What does this
function tell us about the rule for doing matrix multiplication? Please explain the lines and the function here.

```{r tediousmult,results='hide',tidy=TRUE,tidy.opts=list(keep.comment=FALSE)}
X.times.b <- matrix(NA, nrow = 8, ncol = 1) # Initialize a results matrix
for (row.entry in 1:8) { # explain - ANSWER: repeats for each of 8 rows in the X matrix
  temp <- vector(length = 3) # initialize the temp vector
  for (col.entry in 1:3) { # explain - ANSWER: repeats for each of 3 columns 
    temp[col.entry] <- X[row.entry, col.entry] * b[col.entry, ] # explain - Multiplies each value per column of X by the corresponding value in each of 3 rows in b
  }
  X.times.b[row.entry, ] <- sum(temp) #ANSWER: sums that multiplication for each row in X.times.b
}
X.times.b
```
# Note: When multiplying matrices, the number of rows in the first matrix must match the number of columns in the second matrix.  Here, we are multiplying a 8x3 matrix by a 3x1 vector.  This means that order matters.  The result will be an 8x1 matrix (the number of rows from the first and the number of columns from the second) where each value is the sum of what was multiplied.

Now, doing part of it by hand: Please explain the lines of code.
```{r byhand,results='hide',tidy.opts=list(keep.comment=TRUE)}
(X[1, 1] * b[1]) + (X[1, 2] * b[2]) + (X[1, 3] * b[3]) ## (the number in the first row and first column of X times the number in the first row of b) plus (the number in the first row and second column of X times the number in the second row of b) plus (the number in the first row and third column of X times the number in the 3rd row of b)
(X[2, 1] * b[1]) + (X[2, 2] * b[2]) + (X[2, 3] * b[3]) ## (the number in the second row and first column of X times the number in the first row of b) plus (the number in the second row and second column of X times the number in the second row of b) plus (the number in the second row and third column of X times the number in the 3rd row of b)
(X[3, 1] * b[1]) + (X[3, 2] * b[2]) + (X[3, 3] * b[3]) ## (the number in the third row and first column of X times the number in the first row of b) plus (the number in the third row and second column of X times the number in the second row of b) plus (the number in the third row and third column of X times the number in the 3rd row of b)

## etc.... for each row in X
```


And now a little faster (multiplying vectors rather than scalars and
summing): You can break matrix multiplication into separate vector
multiplication tasks since vector multiplication also goes
sum-of-row-times-column. Please explain all of this code.
```{r vectorized,results='hide',tidy.opts=list(keep.comment=FALSE)}
X[1, ] %*% b # Explain - ANSWER: multiplies row 1 of X by the only column in b
X[2, ] %*% b # Explain - ANSWER: repeats for row 2 of X
```

And doing it very fast: This is direct matrix multiplication. So nice
and clean compared to the previous! Don't we love matrix multiplication? - lol yes
  ```{r fast,results='hide'}
X %*% b
```

How does `fitted(thelm)` relate to `X %*% b`? What is `%*%` in  `X %*% b` (often written $\bX \bb$ or $\bX \hat{\bbeta}$)?

# ANSWER: 'fitted(thelm)' gives the predicted y for the model, it is the y value given by the model when we multiply the observed covariate X times the parameters b.
## %*% refers to sum multiplication, which is what we did above with adding the products. “fitted is a generic function which extracts fitted values from objects returned by modeling functions. fitted.values is an alias for it.  All object classes which are returned by model fitting functions should provide a fitted method. (Note that the generic is fitted and not fitted.values.)” (R, ?fitted) -- fitted(thelm) returns the same values as X %*% b.  The difference is that X %*% b returns an 8x1 matrix and fitted(thelm) returns those same 8 values, but they are listed all in the same row with columns titled “1, 2, 3, 4, 5, 6, 7, 8.”

# Also: The ‘fitted’ function pulls the fitted values, aka the predicted values, from a regression model. Essentially, it takes the coefficients for each control variable in the model and multiplies them by the actual value of those variables for the observations in the dataset. If I understand this correctly, it is basically doing the same thing as the matrix multiplication, but starting from the other end because it works from the already calculated ‘b’ coefficients. 

  
  ```{r lmstuff}
thelm <- lm(rpre ~ medhhi1000 + medage, data = news.df)
fitted(thelm)
```


4. How would you use matrix addition/subtraction to get the residuals once
you had $\bX \bb$ (aka `X %*% b`)?

# ANSWER: subtract observed y' = rpre from y = X %*% b -- Residuals are the difference between the actual value and the predicted value.  They can be used to measure how accurate a prediction is by seeing how close that prediction is to the actual value. In this case, X%*%b is the expected values of voter turnout based off our model using the explanatory variables of income, age, diversity, and number of candidates running.  We can compare this to news.df$r, which has the actual voter turnout values. -- news.df$r - (X %*% b)

  
  5. Now, let's meet another important matrix:

```{r xtx,error=TRUE,warning=TRUE,message=TRUE}
# Now another important matrix:
try(X %*% X) # why doesn't this work. - ANSWER: the columns of X do not equal the rows, and matrix multiplication is column by row.
```

```{r xtx2,results='hide'}
t(X) # What is this? - ANSWER: transpose X, it flips the values across the diagonal
XtX <- t(X) %*% X # What is XtX - ANSWER: this is transpose X times X, which produces a matrix that is symmetrical about the diagonal with Row 1 containing the values for distributing each row times itself. ##Now we can multiply because we are multiplying a 3x8 matrix by an 8x3 matrix.  XtX is part of the ordinary least squares equation and “contains sums of squares and cross products among the regressors” (Fox 2008, chapter 9).  Squared deviance bc deviance of the mean times itself? -- NOTE: Another potential explanation? Based on my reading, it seems as though, in this matrix, the diagonal values represent the sum of squares for the variance, while the non-diagonal values speak to, in some degree, the relationship between the variables, which (I think) would be covariances. 

XtX
```

To make our lives easier, let's mean deviate or center or align all of the variables
(set it up so that all of them have mean=0). Now the `XtX`
matrix will be easier to understand:


```{r}
colmeansvec <- colMeans(X) # What does this do? - This creates a vector from the averages of each column.
colmeansmat <- matrix(rep(colmeansvec, 8), ncol = 3, byrow = TRUE)
# Fill a matrix with those means: repeat each row 8 times and stack them
```

Why would we want a matrix like the following?
```{r meancent,echo=TRUE,results='hide'}
X - colmeansmat
# Subtract the column means from each element of X - This is helpful because then we can calculate how far each value deviates from the mean.
# What is happening here? - Gives deviance, i.e. This gives the difference between each value of X and the mean for its row (which was its covariate). It gives the distance of the measured covariate from its mean, which could then be used to find standard deviation and dispersion. Another way to think of this: this function "centers" the covariates with respect to their means, eliminating 
# Or here?
t(apply(X, 1, function(x) {
  x - colmeansvec
})) ## Apply: “Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix” (R). --  ‘apply’ here is going to apply a function to a matrix. In this case, it is going to apply to matrix ‘X’, in its rows (denoted by ‘1’), a function ‘x’ where each of the values in the row has the ‘colmeansvec’ value subtracted from it.

# And here is another way to do it:
sweep(X, 2, colmeansvec) # See the help page for sweep - "sweeps out" the colmeans from the matrix --  “Return an array obtained from an input array by sweeping out a summary statistic” (R). -- Also: I think this does the same thing as above, but simplified because rather than specifying a function internally, the ‘sweep’ function defaults to subtraction? 

```

Please explain this code:

```{r xtx3,results='hide'}
X.md <- X - colmeansmat # gives deviance of X - how much the coefficients of the explanatory variables in matrix X differ from the mean
y.md <- y - mean(y) # dispersion of y - how much the voter turnout in each city differs from the mean
XtX.md <- t(X.md) %*% X.md # gives the transpose of the deviation (distance from mean) times the deviation
XtX.md # the diagonal gives the sum of deviations squared, (t(X.md)[2,] %*% X[,2]), the 2798.95 is the sum of co-deviation squared (the value prior to finding covariance) - this matrix multiplies the transposition of X.md by X.md, which has the differences between actual and mean
```

Explain what each entry in `XtX.md` is: if you can, relate those numbers to
quantities like variances, covariances, sums (of squares or not), sample sizes, do so.

# Answer: each value gives the "deviation squared", which is the step before we find variance/covariance. This gives the sum of deviation times itself or time the deviation of the other covariates, i.e. (“X_1 * X_1” , “X_1 * X_2” , etc). The diagonal gives the squared prior to variance and the other terms give the squared prior to covariance (when we average these terms)

Here another representation of `XtX` that might help you explain and reproduce
the entries above where $x_{1i}$ and $x_{2i}$ represent the two covariates in
our prediction model.

$$\bX^{T}\bX=\begin{bmatrix} n & \sum_{i = 1}^{n}{x_{1i}} & \sum_{i =
    1}^{n}{x_{2i}} \cr \sum_{i = 1}^{n}{x_{1i}} & \sum_{i = 1}^{n}
  {{x_{1i}}}^2 & \sum_{i = 1}^{n}{x_{1i}}{x_{2i}} \\ \sum_{i =
    1}^{n}{x_{2i}} & \sum_{i = 1}^{n} {x_{1i}}{x_{2i}} & \sum_{i =
    1}^{n}{{x_{2i}}}^2 \end{bmatrix}$$

Try some of the following commands to get some other help in understanding what XtX is:

```{r XtX,results='hide',tidy.opts=list(keep.comment=FALSE)}
##as a reminder, X.md is an 8x3 matrix containing the differences between the values in the X matrix and their means
sum(X.md[, 1]) ## This is taking the sum of all the values in the first column
sum(X.md[, 2]^2)  ## This is taking the sum of all the squared values in the second column.  We are most likely squaring here because some of the differences are positive and some are negative - if we added them as is they would start to cancel out, but the squared differences will add together.  
sum((X.md[, 2] - mean(X.md[, 2]))^2) ## This is the variance of the second column
sum((X.md[, 2] - mean(X.md[, 2]))^2) / (8 - 1) #
var(X.md[, 2]) # Another way to get variance of z
sum(X.md[, 2] * X.md[, 3]) # why not use %*%? (ans: we want the cross-product for the covariance)
sum(X.md[, 2] * X.md[, 3]) / (8 - 1)  ## Here we are dividing the variance by n-1 to get the sample variance - (we also, perhaps mistakenly, call this "covariation" elsewhere in this assignment because we are slightly confused about terminology. Do these things mean the same thing, because we seem to find different vocabulary for the same formulas ... )
cov(X.md) # see the help file on cov(): ## “var, cov and cor compute the variance of x and the covariance or correlation of x and y if these are vectors. If x and y are matrices then the covariances (or correlations) between the columns of x and the columns of y are computed” (R).
XtX.md / (8 - 1) # What is this? # the diagonals give the covariance of the covariates
```

What about $\bX^{T} \by$? Explain the entries in `Xty.md`. Explain the code.

```{r Xty,results='hide',tidy.opts=list(keep.comment=FALSE)}
t(X.md) ## transposing X.md
y.md
Xty.md <- t(X.md) %*% y.md ##multiplying the transposed X.md matric by the y.md matrix
Xty.md # What is this? # sum of co-deviation squared.  -- This is the other part of the ordinary least squares equation that “contains sums of cross products between the regressors and the response variable” (Fox 2008, Ch 9).
cov(cbind(X.md, y.md)) ###cbind() combines the columns of this 8x3 matrix and 8x1 matrix, resulting in a new 8x4 matrix
##cov() takes the covariance/correlations between the columns of x and y.  A positive covariance means that the variables are positively/directly correlated and a negative covariance means that the variables are negatively/inversely correlated.

Xty.md / 7 # diagonals give the covariance of the covariates
```

The following is a verbal formula for a covariance:
deviations of x from its mean times deviations of y from its mean
divided by n-1 (roughly the average of the product of the deviations)
```{r xty2,results='hide'}
sum((X[, 2] - mean(X[, 2])) * (y - mean(y))) # sum of deviations for 2nd row of X by y
sum((X[, 2] - mean(X[, 2])) * (y - mean(y))) / (8 - 1) # covariance of 2nd row of X by y
# Same as above because we've removed the means from X.md and y.md
sum((X.md[, 2]) * (y.md)) / (8 - 1)
Xty <- t(X) %*% y
Xty
Xty / (8 - 1) # this is like the covariance equation, except it would be for a mean of zero ...
```

4. And finally division: Try each of the following lines of math in
R and explain to yourself (or your colleagues) what happened
(ideally relate what happened to ideas about variances, covariances,
  sums, etc..)

```{r div,results='hide',tidy.opts=list(keep.comment=FALSE)}
X / 2 # Explain - divides all X values by 2
1 / X # Explain - produces inverse of each individual value in X
X^(-1) # Same as above: 1/X==(X^(-1))
1 / X == (X^(-1))
# Now matrix inversion using solve()
try(solve(X)) # Doesn't work --- why? - solve only works for square matrices -- ## If we try solve(X) we can an error message saying that the matrix must be square --  Solve = “This generic function solves the equation a %*% x = b for x, where b can be either a vector or a matrix” (R).   ## https://courses.lumenlearning.com/boundless-algebra/chapter/inverses-of-matrices/#:~:text=The%20definition%20of%20a%20matrix,must%20be%20square%20as%20well : “The definition of a matrix inverse requires commutativity—the multiplication must work the same in either order.  To be invertible, a matrix must be square, because the identity matrix must be square as well.”
solve(XtX) # This works, why? - because XtX is square
dim(XtX)
dim(Xty)
solve(XtX) %*% Xty # WOW! AMAZING! WONDERFUL! (what is this?) - this gives the coefficients, aka the ordinary least squares estimator (Fox)
try(solve(XtX.md)) #  What is the problem? - inverses for matrices with either a row or column of all zeros cannot be found, they are "dimension crushing" matrices or "trap door" matrices. -- Also: XtX.md is a square matrix, so that is not the issue.  We are getting an error: “Lapack routine dgesv: system is exactly singular: U[1,1] = 0.”  This is saying that the matrix is singular and cannot be converted.  A singular matrix has a determinant of 0.
XtX.md <- t(X.md[, 2:3]) %*% X.md[, 2:3] # Hmm? What is happening? - this gives the sum of deviations squared -- this is removing the 0 values from the first row and first column of the matrix, thus removing the matrix values which made it a singular matrix. Thus, the solve command should work now. 
try(solve(XtX.md))
Xty.md <- t(X.md[, 2:3]) %*% y.md
solve(XtX.md) %*% Xty.md ## These are the values from the ‘wow amazing’ line above, minus the first entry (which I think is the intercept)
```

```{r div2}
# Notice that this is not the same as
(1 / XtX) %*% Xty # Matrix inversion is different from scalar division
# How does this help us?
lm(I(rpre - mean(rpre)) ~ I(medhhi1000 - mean(medhhi1000)) + I(medage - mean(medage)) - 1, data = news.df) # the "I" allows the operators to be treated as arithmetic operators instead of formula operators. This also shows that the coefficients for the deviations is the same as the coefficients for the covariates and response. Since this regression uses deviance, the y intercept is zero (set to the deviance of mean of y) --  This is taking a linear regression where we have the difference between the predicted turnout and the mean predicted turnout ~ two of the explanatory variables (income and age) the difference between their value and the mean of their values
# or rpre
lm(scale(rpre, scale = FALSE) ~ scale(medhhi1000, scale = FALSE) + scale(medage, scale = FALSE) - 1, data = news.df) # same as above, uses deviations 
# also: “scale is generic function whose default method centers and/or scales the columns of a numeric matrix” (R).  This gives us the same coefficients for median income and median age as the previous linear regression.  Rescales it to the mean.
lm(y.md ~ X.md[, 2] + X.md[, 3] - 1)
coef(lm(rpre ~ medhhi1000 + medage, data = news.df)) # same as above, except "re-centered" to the intercept based on the observations and not the deviance.
```
6. So, the vector of least squares coefficients is the result of
dividing what kind of matrix by what kind of matrix? (what kind of
                                                      information by what kind of information)? \emph{Hint:} This
perspective of "accounting for covariation" is another valid way
to think about what least squares is doing [in addition to smoothing
                                            conditional means]. They are mathematically equivalent.
                                            
# ANSWER： The vector of least squares coefficients is given by dividing the sums of squares of variances (Xty) by the sums of squares of covariances (XtX). We are slightly confused about terminology though, and had a bit of a debate around when “covariance” is used and when “variance” is used. Some in our group thought that “variance” would be used for “X_n * y” and others in our group thought “variance” is only for “X_n * X_n” or “y_n * y_n” (i.e. the same covariate or response times itself, and “co-” is used for differing covariates or covariates * responses. Would you please clarify the definition in class? This is slightly confusing also because "var(x) = cov(x,x)" per the equation below?

# Also, the vector of least squares coefficients solves for the coefficient by using the derivative of the residual sum of squares in order to solve for the local minimum of the equation, i.e. when the slope is zero. This finds the coefficient at the local minimum of the RSS.

Why should covariances divided by variances amount to differences of
means, let alone adjusted differences of means?

# ANSWER: Covariance is the sum of the differences in means for x and differences in means for y divided by n-1.  Variance is the sum of the differences in means for x.  Covariance, therefore, is just variance multiplied by the differences in means for y.  So when you divide them, the differences in means for y is what is left. 

  
  Here are some definitions of covariance and variance:
  
  $$\cov(X,Y)=\frac{\sum_{i}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{n-1} $$
  $$\var(X)=\cov(X,X)=\frac{\sum_{i}^{n}(X_i - \bar{X})(X_i - \bar{X})}{n-1}=\frac{\sum_{i}^{n}(X_i - \bar{X})^2}{n-1}
$$
  
  So, first,

$$\frac{\cov(X,Y)}{\var(X)}=\frac{\sum_{i}^{n}(X_i -
                                                 \bar{X})(Y_i - \bar{Y})}{\sum_{i}^{n}(X_i - \bar{X})^2}$$
  
  because
the (n-1) cancels out. (Thus, we had to divide $X^{T}X$ and $X^{T}y$
                          by n-1 in the sections above to get the analogous
                        covariances/variances). So, this is the bivariate case with
$y=\beta_0+\beta_1 x_1$. What about $y=\beta_0+\beta_1 x_1+ \beta_2 x_2$?  
## that is the equation for the covariance between x and y 

  
  This becomes notationally messy fast.  Already,
however, you can get a sense for the idea of deviations from the mean
being a key ingredient in these calculations.

7.  Why might $\bX \boldsymbol{\beta}$ be useful? Let's get back to the question of prediction. So far we have the a model that predicts future turnout with the following squared error. Please explain the following code.

```{r}
X <- as.matrix(cbind(constant = 1, news.df[, c("medhhi1000", "medage", "blkpct", "cands")])) ## X is a matrix with the first column being the constant 1 and each of the other columns being the values in each city for the 4 different explanatory variables.
y <- news.df$rpre # rpre are the predicted voter turnouts, which is the response variable y
bVector <- solve(t(X) %*% X) %*% t(X) %*% y # the bVector is the ordinary least squares estimator

yhat <- X %*% bVector # the predicted y values based off our linear model is the X matrix of the explanatory variables times the ordinary least squares estimator B

errors <- news.df$r - yhat # the errors are the difference between the actual voter turnout and what our model predicted

summary(errors) # the mean error was 3.125 which means that the average error had the actual voter turnout as around 3 more people than the predicted turnout.  The median error was -1.058 which means the median error had the actual turnout as around 1 person less than the predicted turnout.

mseOLS <- mean(errors^2) # the mean squared error of the ordinary least squares regression takes the mean of the squared errors

```

Now, we suspect that we could do better than this from @james2013introduction. Here is an example using the lasso. What do you think? Did we do a better job than OLS in this small dataset? What is going on in this code? Please explain the code.

# LASSO (least absolute selection shrinkage operator:
# Coursera
# Shrinkage (constraint on parameters that shrinks coefficients towards zero) and variable selection (identifies the most important variables associated with the response variable) in linear regression
# Why lasso vs. OLS?
# Greater predictor accuracy
# Increase model interpretability

# James et al. (2013)
# Why would we use something besides OLS? (James et al. 2013, 203-204)
# Prediction Accuracy                        
# Model Interpretability
# What are our options? 
# Subset selection (least squares)
# Shrinkage 
# Ridge regression or LASSO
# “we can fit a model containing all p predictors using a technique that constrains # or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient towards zero” (James et al. 2013, 214)     
# “the lasso shrinks each least squares coefficient towards zero by a constant amount, λ/2; the least squares coefficients that are less than λ/2 in absolute value are shrunken entirely to zero” (James et al. 2013, 225).
# If λ is small then the amount of shrinkage needed is similar to OLS, so we might just want to use OLS in those cases (James et al. 2013, 227-228).
# Dimension Reduction 

## NOTE: this is a method to remove covariates (shrink them to zero) depending on the size of lamda. Per Breiman (2001), this is "old style stats" where the desired model has fewer covariates and thus is "easier to interpret" because the resultant model has a smaller set of covariates that are interpreted as doing "most/more of the work" in the prediction. Breiman would critique this approach because it "loses information" through the lamda coefficient shrinking process (i.e. dimension reduction). This does avoid the "curse of dimensionality," but as we begin to think about alternative methods for analyzing data, not all models are "subject" to this "curse." Per discussion in class, one goal we should be working toward is always keeping as much information as possible. In this case, it seems that LASSO might not be ideal for that goal.


```{r}
lassoCriterion <- function(lambda, b, y, X) {
  ## Assumes that the first column of X is all 1s
  yhat <- X %*% b
  ehat <- y - yhat
  l1.penalty <- sum(abs(b[-1])) ## no penalty on intercept
  thessr <- sum(ehat^2)
  lassocrit <- thessr + lambda * l1.penalty
  return(lassocrit)
}
## the above function looks similar to the linear model from before, but is adding a penalty.  Lassos penalize extra features in the model that don’t help the regression results enough by shrinking them to smaller values, potentially as small as zero (Udacity Youtube)

## Associated with each value of λ is a vector of ridge regression coefficients
lassoCriterion(lambda = .5, b = rep(0, ncol(X)), y = y, X = X)
lassoSol <- optim(
  par = c(0, 0, 0, 0, 0),
  fn = lassoCriterion,
  method = "BFGS",
  X = X,
  y = y,
  lambda = .5,
  control = list(trace = 1, REPORT = 1)
)
# Now we are applying the Lasso to our data.

yhatL1 <- X %*% lassoSol$par
errorsL1 <- news.df$r - yhatL1
mseL1 <- mean(errorsL1^2)
## Now see if we can find the best value of lambda
nlambdas <- 100
somelambdas <- seq(.001, 100, length = nlambdas)
## A function to get lasso criterion coefs and MSE
getlassoMSE <- function(lambda, X, y) {
  lassoSol <- optim(
    par = rep(0, ncol(X)),
    fn = lassoCriterion,
    method = "BFGS",
    X = X,
    y = y,
    lambda = lambda
  )
  # ,control=list(trace=1,REPORT=1))
  yhatL1 <- X %*% lassoSol$par
  errorsL1 <- news.df$r - yhatL1
  mseL1 <- mean(errorsL1^2)
  return(c(par = lassoSol$par, mse = mseL1, lambda = lambda))
}
results <- sapply(somelambdas, function(l) {
  getlassoMSE(l, X = X, y = y)
})
## apply(results,1,summary)
min(results["mse", ]) > mseOLS # ANSWER: why check this? Maybe because if the results of the lasso are every smaller than the mseOLS then there's some major problem with the analysis? The mse of the lasso is supposed to approach the mseOLS as lambda decreases. 
results["lambda", results["mse", ] == min(results["mse", ])]
## NOTE: according to James et al., glmnet() will approach the exact least squares coefficients as lambda approaches zero, since there is no "shrinkage penalty" when lambda is zero. This appears to be correct, as the results show ... 
```

```{r, eval=FALSE}
## To do this even faster:
library(glmnet)
lassoFits <- glmnet(x = X[, -1], y = y, alpha = .5) ## using an elastic net fit rather than strict lasso
par(mar = c(6, 2, 3, 1), mgp = c(1.5, .5, 0))
plot(lassoFits, xvar = "lambda", label = TRUE)
## This next line add the raw lambdas since the default plot shows only the log transformed lambdas
axis(1, line = 2, at = log(lassoFits$lambda), labels = round(lassoFits$lambda, 3)) # this suggests that as lambda approaches 2 many of the coefficients approach zero, which would allow for variable selection, if that were something we were hoping to accomplish (though presently unsure, per Breiman, 2001)
abline(h = 0, col = "gray", lwd = .5)

## James et al also recommends using cross-validation to find the lambda with smallest cross-validation error. Below is an attempt at this:
set.seed(1)
lassocv.glm <- cv.glmnet(x = X[, -1], y = y, alpha = .5) ## using an elastic net fit rather than strict lasso
bestlamtest <- lassocv.glm$lambda.min
bestlamtest # this suggests a lambda of 10.04 would yield the least cv error. Thus, there is less variance at lambda~10, with a slight increase in bias, per the "bias-variance trade-off" (James et al, 2013, p. 217).
```

```{r eval=FALSE}
getMSEs <- function(B, X, obsy) {
  yhats <- apply(B, 2, function(b) {
    X %*% b
  })
  ehats <- apply(yhats, 2, function(y) {
    obsy - y
  })
  apply(ehats, 2, function(e) {
    mean(e^2)
  })
}
getMSEs(B = coef(lassoFits), X = X, obsy = y)

```

Hmmm.... should the MSE always just go down? I wonder what @james2013introduction has to say about this.^[I suspect that @james2013introduction would recommend cross-validation --- but we only have 8 observations here, so that would be difficult.]

# ANSWER: this is an optimizing function. The MSE should go down until it finds the absolute/relative minimum of the function where a change in any parameter will increase the mean standard error. This function appears to be finding 109.07 as the MSE. This is approximately the same as the value we found in the earlier optim model of 873.68. In this case, since we're finding "mean" standard error, the lasso is reporting 873.68/8. -- James et al. (2013) say that the benefits of using fitting models besides OLS are (a) to increase prediction accuracy and (b) to increase model interpretability).  Increasing prediction accuracy should mean that MSE is going down.  Lasso can increase prediction accuracy by shrinking coefficients that are not as relevant to the model. This again seems contrary to the Breiman article, and so there appears to be controversy around approaches. Breiman would say that reducing dimensions is bad for prediction, but perhaps it is good for prediction within certain modeling approaches.

# Cross-validation is a resampling method.  “It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.... The goal of cross-validation is to test the model’s ability to predict new data that were not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset” (Audioversity).  In this case, new data that was not used to estimate the model I believe would be other cities not included in the news.df dataset.  We only have 8 observations, so how well would that be able to predict a multitude of other cities when our model is based on a small number of cities? --- One of the best options would be to use "leave-one-out cross-validation" (James et al., 2013, p. 178) which attempts to address the drawback of setting aside too many observations for the test set. Thought this is "unbiased for test error" (ibid, p. 179), it is a highly variable and thus a poor estimate since it is based on only one single observation. If we repeat this process n times for n observation (8 in this case) and then average these, we arrive at the LOOCV estimate. This tends to not overestimate the test error rate as much as the approach using a validation set and does not yield multiple results due to randomness in the set splits since it averages the the repeated process for all n testable observations. We could run a similar test with "k-Fold Cross Validation" which would also average the results. Given the small starting sample though, it is probably not as accurate as LOOCV.

Now, more benefits of penalized models. Imagine this model:

```{r}
newmodel <- rpre ~ blkpct * medhhi1000 * medage * cands
lm4 <- lm(newmodel, news.df)
X <- model.matrix(newmodel, data = news.df) ## per James et al. (2013), model.matrix is useful because it automatically transforms qualitative variables into dummy variables and produces a matrix corresponding to the number of predictors. 
try(b <- solve(t(X) %*% X) %*% X %*% y)
lsCriterion2 <- function(b, y, X) {
  yhat <- X %*% b
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}
lsSolution1 <- optim(fn = lsCriterion2, par = rep(0, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
lsSolution1
lsSolution2 <- optim(fn = lsCriterion2, par = rep(100, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
lsSolution2
lsSolution3 <- optim(fn = lsCriterion2, par = rep(-100, ncol(X)), X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000)) ## Question: not exactly sure why the changed parameters cause such a big difference ...
lsSolution3
cbind(lsSolution1$par, lsSolution2$par, lsSolution3$par)
## Notice that we are not standardizing the columns of X here. Should do that for real use.
ridgeCriterion <- function(lambda, b, y, X) {
  ## Assumes that the first column of X is all 1s
  yhat <- X %*% b
  ehat <- y - yhat
  l2.penalty <- sum(b[-1]^2) ## no penalty on intercept
  thessr <- sum(ehat^2)
  lassocrit <- thessr + lambda * l2.penalty
  return(lassocrit)
}
ridgeSolution1 <- optim(fn = ridgeCriterion, par = rep(0, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
ridgeSolution1
ridgeSolution2 <- optim(fn = ridgeCriterion, par = rep(100, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000)) # not sure why the changed parameters with the ridgeCriterion function are changing the output, because of the penalty? Since lsSolution1 and lsSolution2 both have the same parameters, wouldn't they have the same SSRs? And, since both ridgeSolution 1 & 2 have the same lambdas, why is the lassocrit turning out different between the 1st and 2nd ridgeSolution? Isn't the "shrinkage penalty" the same for both since they have the same lambda and same coefficients? Or does the rep() function somehow make the coefficients bigger? -- Also it appears that the intercept has been penalized for ridgeSolution2 and 3, is this true?
ridgeSolution2
ridgeSolution3 <- optim(fn = ridgeCriterion, par = rep(-100, ncol(X)), lambda = .5, X = X, y = y, method = "BFGS", control = list(trace = 1, REPORT = 1, maxit = 5000))
ridgeSolution3
cbind(ridgeSolution1$par, ridgeSolution2$par, ridgeSolution3$par)
## A sketch of Cross-validation to choose lambda: here using 3-fold because of the small dataset
cvfn <- function() {
  testids <- sample(1:nrow(X), nrow(X) / 3)
  trainingids <- (1:nrow(X))[-testids]
  ## Fit
  ## Predict yhat for testids
  ## MSE for y_test versus yhat_test
}
## Average of the MSE across folds is CV MSE
## code below is pasted from earlier, it seems we could use something like this for in cvfn? 
set.seed(1)
lassocv.glm <- cv.glmnet(x = X[, -1], y = y, alpha = .5) ## using an elastic net fit rather than strict lasso
bestlamtest <- lassocv.glm$lambda.min
bestlamtest # this suggests a lambda of 10.04 would yield the least cv error
```


# References

